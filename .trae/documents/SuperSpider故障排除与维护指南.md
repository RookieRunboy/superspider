# SuperSpider æ•…éšœæ’é™¤ä¸ç»´æŠ¤æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [å¸¸è§é—®é¢˜æ’æŸ¥](#å¸¸è§é—®é¢˜æ’æŸ¥)
2. [é”™è¯¯ä»£ç è¯´æ˜](#é”™è¯¯ä»£ç è¯´æ˜)
3. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
4. [æ—¥å¿—åˆ†æ](#æ—¥å¿—åˆ†æ)
5. [ç³»ç»Ÿç»´æŠ¤](#ç³»ç»Ÿç»´æŠ¤)
6. [ç›‘æ§æŒ‡æ ‡](#ç›‘æ§æŒ‡æ ‡)

## ğŸ” å¸¸è§é—®é¢˜æ’æŸ¥

### 1.1 Excelæ–‡ä»¶ç›¸å…³é—®é¢˜

#### é—®é¢˜ï¼šExcelæ–‡ä»¶è¯»å–å¤±è´¥
**ç—‡çŠ¶**ï¼š
```
FileError: Excelæ–‡ä»¶ä¸å­˜åœ¨: input/urls.xlsx
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. æ£€æŸ¥æ–‡ä»¶æƒé™
4. éªŒè¯æ–‡ä»¶æ ¼å¼ï¼ˆ.xlsx, .xlsï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la input/

# æ£€æŸ¥æ–‡ä»¶æƒé™
chmod 644 input/urls.xlsx

# éªŒè¯æ–‡ä»¶æ ¼å¼
file input/urls.xlsx
```

#### é—®é¢˜ï¼šURLåˆ—è¯†åˆ«å¤±è´¥
**ç—‡çŠ¶**ï¼š
```
FileError: Excelæ–‡ä»¶ä¸­æœªæ‰¾åˆ°URLåˆ—
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥Excelæ–‡ä»¶ä¸­çš„åˆ—å
2. ç¡®è®¤URLåˆ—æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®
3. æ£€æŸ¥åˆ—åæ˜¯å¦ç¬¦åˆè§„èŒƒ

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ”¯æŒçš„URLåˆ—åï¼š`url`, `URL`, `link`, `é“¾æ¥`, `ç½‘å€`
- ç¡®ä¿URLåˆ—åŒ…å«æœ‰æ•ˆçš„ç½‘å€æ•°æ®
- å¦‚æœåˆ—åä¸ç¬¦åˆè§„èŒƒï¼Œè¯·é‡å‘½ååˆ—

### 1.2 ç½‘ç»œè¿æ¥é—®é¢˜

#### é—®é¢˜ï¼šç½‘ç»œè¯·æ±‚è¶…æ—¶
**ç—‡çŠ¶**ï¼š
```
NetworkError: è¯·æ±‚è¶…æ—¶: https://example.com
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€
2. æµ‹è¯•ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®
3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
4. éªŒè¯ä»£ç†é…ç½®

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æµ‹è¯•ç½‘ç»œè¿æ¥
ping google.com

# æµ‹è¯•ç›®æ ‡ç½‘ç«™
curl -I https://example.com

# å¢åŠ è¶…æ—¶æ—¶é—´
python superspider.py --timeout 60

# å‡å°‘å¹¶å‘æ•°
python superspider.py --concurrent 3
```

#### é—®é¢˜ï¼šSSLè¯ä¹¦éªŒè¯å¤±è´¥
**ç—‡çŠ¶**ï¼š
```
SSLError: [SSL: CERTIFICATE_VERIFY_FAILED]
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨web_parser.pyä¸­æ·»åŠ SSLé…ç½®
import ssl
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.ssl_ import create_urllib3_context

# åˆ›å»ºè‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
ctx = create_urllib3_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE
```

### 1.3 ç¼–ç é—®é¢˜

#### é—®é¢˜ï¼šä¸­æ–‡å­—ç¬¦æ˜¾ç¤ºä¹±ç 
**ç—‡çŠ¶**ï¼šPDFæˆ–æ—¥å¿—ä¸­å‡ºç°ä¹±ç å­—ç¬¦

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ç³»ç»Ÿå­—ä½“å®‰è£…
2. éªŒè¯ç½‘é¡µåŸå§‹ç¼–ç 
3. æŸ¥çœ‹ç¼–ç æ£€æµ‹æ—¥å¿—

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆUbuntu/Debianï¼‰
sudo apt-get install fonts-wqy-zenhei fonts-wqy-microhei

# å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆCentOS/RHELï¼‰
sudo yum install wqy-zenhei-fonts wqy-microhei-fonts

# macOSå®‰è£…å­—ä½“
brew install font-wqy-zenhei
```

### 1.4 PDFç”Ÿæˆé—®é¢˜

#### é—®é¢˜ï¼šPDFç”Ÿæˆå¤±è´¥
**ç—‡çŠ¶**ï¼š
```
PDFError: PDFç”Ÿæˆå¤±è´¥: å­—ä½“åŠ è½½é”™è¯¯
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ReportLabå®‰è£…
2. éªŒè¯å­—ä½“æ–‡ä»¶å­˜åœ¨
3. æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨pdf_generator.pyä¸­æ·»åŠ å­—ä½“æ£€æŸ¥
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

def check_fonts():
    """æ£€æŸ¥å­—ä½“æ˜¯å¦å¯ç”¨"""
    try:
        # æ³¨å†Œå­—ä½“
        pdfmetrics.registerFont(TTFont('SimHei', 'SimHei.ttf'))
        return True
    except Exception as e:
        print(f"å­—ä½“åŠ è½½å¤±è´¥: {e}")
        return False
```

### 1.5 å†…å­˜å’Œæ€§èƒ½é—®é¢˜

#### é—®é¢˜ï¼šå†…å­˜ä½¿ç”¨è¿‡é«˜
**ç—‡çŠ¶**ï¼šç³»ç»Ÿå†…å­˜å ç”¨æŒç»­å¢é•¿ï¼Œå¯èƒ½å¯¼è‡´ç¨‹åºå´©æºƒ

**æ’æŸ¥æ­¥éª¤**ï¼š
1. ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
2. æ£€æŸ¥å¹¶å‘æ•°è®¾ç½®
3. åˆ†æå¤„ç†çš„æ–‡ä»¶å¤§å°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
top -p $(pgrep -f superspider)

# å‡å°‘å¹¶å‘æ•°
python superspider.py --concurrent 3

# åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶
# å°†å¤§çš„Excelæ–‡ä»¶æ‹†åˆ†æˆå¤šä¸ªå°æ–‡ä»¶
```

```python
# åœ¨ä»£ç ä¸­æ·»åŠ å†…å­˜ç›‘æ§
import psutil
import gc

def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.2f} MB")
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
```

## ğŸ“Š é”™è¯¯ä»£ç è¯´æ˜

### 2.1 é”™è¯¯åˆ†ç±»

| é”™è¯¯ä»£ç  | é”™è¯¯ç±»å‹ | æè¿° | è§£å†³æ–¹æ¡ˆ |
|----------|----------|------|----------|
| E001 | FileError | Excelæ–‡ä»¶ä¸å­˜åœ¨ | æ£€æŸ¥æ–‡ä»¶è·¯å¾„ |
| E002 | FileError | Excelæ–‡ä»¶æ ¼å¼é”™è¯¯ | ç¡®è®¤æ–‡ä»¶æ ¼å¼ |
| E003 | FileError | URLåˆ—æœªæ‰¾åˆ° | æ£€æŸ¥åˆ—åè§„èŒƒ |
| N001 | NetworkError | è¿æ¥è¶…æ—¶ | å¢åŠ è¶…æ—¶æ—¶é—´ |
| N002 | NetworkError | DNSè§£æå¤±è´¥ | æ£€æŸ¥ç½‘ç»œè®¾ç½® |
| N003 | NetworkError | SSLè¯ä¹¦é”™è¯¯ | é…ç½®SSLè®¾ç½® |
| P001 | ParseError | ç½‘é¡µè§£æå¤±è´¥ | æ£€æŸ¥ç½‘é¡µç»“æ„ |
| P002 | ParseError | ç¼–ç æ£€æµ‹å¤±è´¥ | æ‰‹åŠ¨æŒ‡å®šç¼–ç  |
| D001 | DownloadError | æ–‡ä»¶ä¸‹è½½å¤±è´¥ | æ£€æŸ¥æ–‡ä»¶é“¾æ¥ |
| D002 | DownloadError | ç£ç›˜ç©ºé—´ä¸è¶³ | æ¸…ç†ç£ç›˜ç©ºé—´ |
| PDF001 | PDFError | å­—ä½“åŠ è½½å¤±è´¥ | å®‰è£…ä¸­æ–‡å­—ä½“ |
| PDF002 | PDFError | å†…å®¹è¿‡é•¿ | åˆ†é¡µå¤„ç† |

### 2.2 é”™è¯¯å¤„ç†æµç¨‹

```mermaid
flowchart TD
    A[å‘ç”Ÿé”™è¯¯] --> B{é”™è¯¯ç±»å‹}
    B -->|FileError| C[æ£€æŸ¥æ–‡ä»¶]
    B -->|NetworkError| D[æ£€æŸ¥ç½‘ç»œ]
    B -->|ParseError| E[æ£€æŸ¥ç½‘é¡µ]
    B -->|DownloadError| F[æ£€æŸ¥ä¸‹è½½]
    B -->|PDFError| G[æ£€æŸ¥PDFè®¾ç½®]
    
    C --> H{å¯ä¿®å¤?}
    D --> H
    E --> H
    F --> H
    G --> H
    
    H -->|æ˜¯| I[åº”ç”¨ä¿®å¤æ–¹æ¡ˆ]
    H -->|å¦| J[è®°å½•é”™è¯¯æ—¥å¿—]
    
    I --> K[é‡è¯•æ“ä½œ]
    J --> L[è·³è¿‡å½“å‰é¡¹]
    
    K --> M{æˆåŠŸ?}
    M -->|æ˜¯| N[ç»§ç»­å¤„ç†]
    M -->|å¦| O[è¾¾åˆ°é‡è¯•ä¸Šé™?]
    
    O -->|æ˜¯| J
    O -->|å¦| K
    
    L --> N
    N --> P[å¤„ç†å®Œæˆ]
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 3.1 å¹¶å‘ä¼˜åŒ–

#### æœ€ä½³å¹¶å‘æ•°è®¾ç½®
```python
# æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è®¾ç½®å¹¶å‘æ•°
import os
import psutil

def get_optimal_concurrent_limit():
    """è·å–æœ€ä½³å¹¶å‘æ•°"""
    cpu_count = os.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    # åŸºäºCPUæ ¸å¿ƒæ•°å’Œå†…å­˜å¤§å°è®¡ç®—
    if memory_gb >= 8:
        return min(cpu_count * 2, 10)
    elif memory_gb >= 4:
        return min(cpu_count, 5)
    else:
        return min(cpu_count // 2, 3)
```

#### è¿æ¥æ± ä¼˜åŒ–
```python
# åœ¨web_parser.pyä¸­ä¼˜åŒ–è¿æ¥æ± 
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class OptimizedSession:
    def __init__(self):
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        # é…ç½®è¿æ¥æ± 
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=20,
            pool_maxsize=20
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
```

### 3.2 å†…å­˜ä¼˜åŒ–

#### æµå¼å¤„ç†å¤§æ–‡ä»¶
```python
def download_large_file(url, filename):
    """æµå¼ä¸‹è½½å¤§æ–‡ä»¶"""
    with requests.get(url, stream=True) as response:
        response.raise_for_status()
        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
```

#### å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶
```python
import tempfile
import shutil
from pathlib import Path

def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    temp_dir = Path(tempfile.gettempdir())
    superspider_temps = temp_dir.glob('superspider_*')
    
    for temp_path in superspider_temps:
        try:
            if temp_path.is_file():
                temp_path.unlink()
            elif temp_path.is_dir():
                shutil.rmtree(temp_path)
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
```

### 3.3 ç¼“å­˜ä¼˜åŒ–

```python
from functools import lru_cache
import hashlib

class WebParserCache:
    """ç½‘é¡µè§£æç¼“å­˜"""
    
    def __init__(self, cache_size=128):
        self.cache = {}
        self.cache_size = cache_size
    
    def get_cache_key(self, url):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def get(self, url):
        """è·å–ç¼“å­˜"""
        key = self.get_cache_key(url)
        return self.cache.get(key)
    
    def set(self, url, content):
        """è®¾ç½®ç¼“å­˜"""
        if len(self.cache) >= self.cache_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        key = self.get_cache_key(url)
        self.cache[key] = content
```

## ğŸ“ æ—¥å¿—åˆ†æ

### 4.1 æ—¥å¿—çº§åˆ«å’Œæ ¼å¼

```python
# æ—¥å¿—é…ç½®ç¤ºä¾‹
LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
        'simple': {
            'format': '%(levelname)s - %(message)s'
        }
    },
    'handlers': {
        'file': {
            'level': 'DEBUG',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'superspider.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'detailed'
        },
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'simple'
        }
    },
    'loggers': {
        'superspider': {
            'handlers': ['file', 'console'],
            'level': 'DEBUG',
            'propagate': False
        }
    }
}
```

### 4.2 å…³é”®æ—¥å¿—åˆ†æ

#### æ€§èƒ½åˆ†æ
```bash
# åˆ†æå¤„ç†æ—¶é—´
grep "å¤„ç†å®Œæˆ" superspider.log | awk '{print $1, $2, $NF}'

# ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "ERROR" superspider.log | awk '{print $4}' | sort | uniq -c

# åˆ†æç½‘ç»œè¯·æ±‚è€—æ—¶
grep "è¯·æ±‚è€—æ—¶" superspider.log | awk '{print $NF}' | sort -n
```

#### é”™è¯¯åˆ†æ
```bash
# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
tail -n 100 superspider.log | grep ERROR

# ç»Ÿè®¡é”™è¯¯é¢‘ç‡
grep ERROR superspider.log | awk '{print $1}' | uniq -c

# åˆ†æç‰¹å®šURLçš„é”™è¯¯
grep "example.com" superspider.log | grep ERROR
```

### 4.3 æ—¥å¿—ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
# log_monitor.py

import re
import sys
from collections import defaultdict, Counter
from datetime import datetime, timedelta

def analyze_log(log_file):
    """åˆ†ææ—¥å¿—æ–‡ä»¶"""
    error_counts = Counter()
    processing_times = []
    url_errors = defaultdict(list)
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            # åˆ†æé”™è¯¯
            if 'ERROR' in line:
                error_type = extract_error_type(line)
                error_counts[error_type] += 1
                
                url = extract_url(line)
                if url:
                    url_errors[url].append(error_type)
            
            # åˆ†æå¤„ç†æ—¶é—´
            if 'å¤„ç†å®Œæˆ' in line:
                time_match = re.search(r'è€—æ—¶: ([\d.]+)ç§’', line)
                if time_match:
                    processing_times.append(float(time_match.group(1)))
    
    # ç”ŸæˆæŠ¥å‘Š
    print("=== é”™è¯¯ç»Ÿè®¡ ===")
    for error_type, count in error_counts.most_common():
        print(f"{error_type}: {count}æ¬¡")
    
    print("\n=== æ€§èƒ½ç»Ÿè®¡ ===")
    if processing_times:
        avg_time = sum(processing_times) / len(processing_times)
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’")
        print(f"æœ€é•¿å¤„ç†æ—¶é—´: {max(processing_times):.2f}ç§’")
        print(f"æœ€çŸ­å¤„ç†æ—¶é—´: {min(processing_times):.2f}ç§’")
    
    print("\n=== é—®é¢˜URL ===")
    for url, errors in url_errors.items():
        if len(errors) > 1:
            print(f"{url}: {len(errors)}ä¸ªé”™è¯¯")

def extract_error_type(line):
    """æå–é”™è¯¯ç±»å‹"""
    if 'NetworkError' in line:
        return 'NetworkError'
    elif 'FileError' in line:
        return 'FileError'
    elif 'ParseError' in line:
        return 'ParseError'
    else:
        return 'Unknown'

def extract_url(line):
    """æå–URL"""
    url_match = re.search(r'https?://[^\s]+', line)
    return url_match.group(0) if url_match else None

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python log_monitor.py <log_file>")
        sys.exit(1)
    
    analyze_log(sys.argv[1])
```

## ğŸ”§ ç³»ç»Ÿç»´æŠ¤

### 5.1 å®šæœŸç»´æŠ¤ä»»åŠ¡

#### æ¯æ—¥ç»´æŠ¤
```bash
#!/bin/bash
# daily_maintenance.sh

# æ¸…ç†è¶…è¿‡7å¤©çš„æ—¥å¿—æ–‡ä»¶
find downloads/ -name "*.log" -mtime +7 -delete

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find /tmp -name "superspider_*" -mtime +1 -delete

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h | grep -E "(downloads|/tmp)" | awk '$5 > 80 {print "è­¦å‘Š: " $6 " ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: " $5}'

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp config.py config.py.backup.$(date +%Y%m%d)
```

#### æ¯å‘¨ç»´æŠ¤
```bash
#!/bin/bash
# weekly_maintenance.sh

# æ›´æ–°ä¾èµ–åŒ…
pip list --outdated

# æ£€æŸ¥ä»£ç è´¨é‡
flake8 *.py
pylint *.py

# è¿è¡Œæµ‹è¯•å¥—ä»¶
pytest tests/

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
python log_monitor.py superspider.log > performance_report.txt
```

### 5.2 å¤‡ä»½ç­–ç•¥

```python
#!/usr/bin/env python3
# backup.py

import os
import shutil
import tarfile
from datetime import datetime
from pathlib import Path

def create_backup():
    """åˆ›å»ºç³»ç»Ÿå¤‡ä»½"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"superspider_backup_{timestamp}.tar.gz"
    
    # è¦å¤‡ä»½çš„æ–‡ä»¶å’Œç›®å½•
    backup_items = [
        'config.py',
        'superspider.py',
        'requirements.txt',
        'input/',
        'downloads/',
    ]
    
    with tarfile.open(backup_name, 'w:gz') as tar:
        for item in backup_items:
            if os.path.exists(item):
                tar.add(item)
                print(f"å·²å¤‡ä»½: {item}")
    
    print(f"å¤‡ä»½å®Œæˆ: {backup_name}")
    return backup_name

def restore_backup(backup_file):
    """æ¢å¤å¤‡ä»½"""
    if not os.path.exists(backup_file):
        print(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        return False
    
    with tarfile.open(backup_file, 'r:gz') as tar:
        tar.extractall()
        print(f"å¤‡ä»½æ¢å¤å®Œæˆ: {backup_file}")
    
    return True

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) == 1:
        create_backup()
    elif len(sys.argv) == 2 and sys.argv[1] == 'restore':
        backup_files = sorted([f for f in os.listdir('.') if f.startswith('superspider_backup_')])
        if backup_files:
            restore_backup(backup_files[-1])  # æ¢å¤æœ€æ–°å¤‡ä»½
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
    else:
        print("ç”¨æ³•: python backup.py [restore]")
```

### 5.3 å¥åº·æ£€æŸ¥

```python
#!/usr/bin/env python3
# health_check.py

import os
import sys
import psutil
import requests
from pathlib import Path

def check_system_health():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    issues = []
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    disk_usage = psutil.disk_usage('.')
    if disk_usage.percent > 90:
        issues.append(f"ç£ç›˜ç©ºé—´ä¸è¶³: {disk_usage.percent:.1f}%")
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    memory = psutil.virtual_memory()
    if memory.percent > 90:
        issues.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory.percent:.1f}%")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = ['config.py', 'superspider.py', 'requirements.txt']
    for file in required_files:
        if not os.path.exists(file):
            issues.append(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}")
    
    # æ£€æŸ¥ç›®å½•æƒé™
    required_dirs = ['input', 'downloads']
    for dir_name in required_dirs:
        if not os.path.exists(dir_name):
            issues.append(f"ç¼ºå°‘ç›®å½•: {dir_name}")
        elif not os.access(dir_name, os.W_OK):
            issues.append(f"ç›®å½•æ— å†™æƒé™: {dir_name}")
    
    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    try:
        response = requests.get('https://www.google.com', timeout=5)
        if response.status_code != 200:
            issues.append("ç½‘ç»œè¿æ¥å¼‚å¸¸")
    except Exception:
        issues.append("ç½‘ç»œè¿æ¥å¤±è´¥")
    
    # è¾“å‡ºç»“æœ
    if issues:
        print("å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues:
            print(f"- {issue}")
        return False
    else:
        print("ç³»ç»Ÿå¥åº·çŠ¶æ€è‰¯å¥½")
        return True

if __name__ == '__main__':
    healthy = check_system_health()
    sys.exit(0 if healthy else 1)
```

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### 6.1 å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | ç›‘æ§æ–¹æ³• |
|------|--------|----------|
| å¤„ç†æˆåŠŸç‡ | >95% | æ—¥å¿—åˆ†æ |
| å¹³å‡å¤„ç†æ—¶é—´ | <30ç§’/URL | æ€§èƒ½æ—¥å¿— |
| å†…å­˜ä½¿ç”¨ç‡ | <80% | ç³»ç»Ÿç›‘æ§ |
| ç£ç›˜ä½¿ç”¨ç‡ | <85% | ç£ç›˜ç›‘æ§ |
| ç½‘ç»œé”™è¯¯ç‡ | <5% | é”™è¯¯æ—¥å¿— |
| PDFç”ŸæˆæˆåŠŸç‡ | >90% | å¤„ç†æŠ¥å‘Š |

### 6.2 ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
# monitor.py

import time
import psutil
import json
from datetime import datetime
from pathlib import Path

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, interval=60):
        self.interval = interval
        self.metrics = []
    
    def collect_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('.').percent,
            'process_count': len(psutil.pids()),
        }
        
        # æ£€æŸ¥SuperSpiderè¿›ç¨‹
        superspider_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            if 'superspider' in proc.info['name'].lower():
                superspider_processes.append(proc.info)
        
        metrics['superspider_processes'] = superspider_processes
        
        return metrics
    
    def save_metrics(self, metrics):
        """ä¿å­˜æŒ‡æ ‡æ•°æ®"""
        metrics_file = Path('metrics.jsonl')
        with open(metrics_file, 'a') as f:
            f.write(json.dumps(metrics) + '\n')
    
    def check_alerts(self, metrics):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        alerts = []
        
        if metrics['cpu_percent'] > 90:
            alerts.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {metrics['cpu_percent']:.1f}%")
        
        if metrics['memory_percent'] > 90:
            alerts.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['memory_percent']:.1f}%")
        
        if metrics['disk_percent'] > 90:
            alerts.append(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['disk_percent']:.1f}%")
        
        return alerts
    
    def run(self):
        """è¿è¡Œç›‘æ§"""
        print(f"å¼€å§‹ç›‘æ§ï¼Œé—´éš”: {self.interval}ç§’")
        
        try:
            while True:
                metrics = self.collect_metrics()
                self.save_metrics(metrics)
                
                alerts = self.check_alerts(metrics)
                if alerts:
                    print(f"[{metrics['timestamp']}] å‘Šè­¦:")
                    for alert in alerts:
                        print(f"  - {alert}")
                
                time.sleep(self.interval)
        
        except KeyboardInterrupt:
            print("\nç›‘æ§å·²åœæ­¢")

if __name__ == '__main__':
    monitor = SystemMonitor(interval=60)
    monitor.run()
```

### 6.3 å‘Šè­¦é…ç½®

```python
# alerts.py

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self, smtp_server, smtp_port, username, password):
        self.smtp_server = smtp_server
        self.smtp_port = smtp_port
        self.username = username
        self.password = password
    
    def send_alert(self, subject, message, recipients):
        """å‘é€å‘Šè­¦é‚®ä»¶"""
        msg = MIMEMultipart()
        msg['From'] = self.username
        msg['To'] = ', '.join(recipients)
        msg['Subject'] = subject
        
        msg.attach(MIMEText(message, 'plain', 'utf-8'))
        
        try:
            server = smtplib.SMTP(self.smtp_server, self.smtp_port)
            server.starttls()
            server.login(self.username, self.password)
            server.send_message(msg)
            server.quit()
            print(f"å‘Šè­¦é‚®ä»¶å·²å‘é€: {subject}")
        except Exception as e:
            print(f"å‘é€å‘Šè­¦é‚®ä»¶å¤±è´¥: {e}")
    
    def check_and_alert(self, metrics):
        """æ£€æŸ¥æŒ‡æ ‡å¹¶å‘é€å‘Šè­¦"""
        alerts = []
        
        # å®šä¹‰å‘Šè­¦è§„åˆ™
        if metrics['memory_percent'] > 90:
            alerts.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['memory_percent']:.1f}%")
        
        if metrics['disk_percent'] > 90:
            alerts.append(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['disk_percent']:.1f}%")
        
        if alerts:
            subject = "SuperSpiderç³»ç»Ÿå‘Šè­¦"
            message = "\n".join(alerts)
            recipients = ['admin@example.com']
            self.send_alert(subject, message, recipients)
```

---

**è¿™ä»½æ•…éšœæ’é™¤ä¸ç»´æŠ¤æŒ‡å—æä¾›äº†å…¨é¢çš„é—®é¢˜è§£å†³æ–¹æ¡ˆå’Œç³»ç»Ÿç»´æŠ¤ç­–ç•¥ï¼Œå¸®åŠ©å›¢é˜Ÿå¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ï¼Œç¡®ä¿SuperSpiderç³»ç»Ÿçš„ç¨³å®šè¿è¡Œã€‚**